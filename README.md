# Few Shot Video
Repository for submitting few shot video work
## Based on:
- https://openaccess.thecvf.com/content_CVPR_2020/papers/Cao_Few-Shot_Video_Classification_via_Temporal_Alignment_CVPR_2020_paper.pdf
- https://arxiv.org/pdf/2110.12358.pdf
- https://github.com/MCG-NJU/FSL-Video
- https://github.com/wangzehui20/OTAM-Video-via-Temporal-Alignment

Complete references in report.pdf
## Summary
Ordered temporal alignment has been shown to outperform meta-learning 
baselines. Surprisingly, a simplistic classifier-based baseline with no temporal alignment 
outperforms metric learning with OTAM, largely attributed to the classifierâ€™s ability to 
exploit dropout. In this repository, I implement and evaluate potential 
improvements to the OTAM method to combine the success of classifier-based methods with 
the success of OTAM relative to meta-baselines by attempting to improve the generalization 
of OTAM. I find that temporal order shuffling augmentation and all-pair similarity loss 
contribution do not provide any significant improvement over OTAM and that reducing the 
number of learnable parameters in the OTAM embedder decreases performance.

## File structure
### Docs
You can find html documentations for all the project code. This is a good place to start if there 
is something you do not understand.
### Scripts
The scripts demonstrate how to call the functionality provided by this repository. You may need 
to change path info. In baseline_evaluate.py, there are a two hard-coded paths declared as global
 constants at the top. I left them that way because it seems more practical given that these will be 
fixed on a given machine. The scripts are intended to be used on the RUG Peregrine cluster.
The module loads will probably not work elsewhere, but you can add the required libraries 
to a virtual environment instead. The script files also activate a virtual environment 
with Torchvision, opencv, numba, tqdm, and a few others. I have a copy of the environment 
in the group folder. Furthermore, the dataset is also in the group folder. Alternatively, you can find it 
here: https://box.nju.edu.cn/d/dc97163752fc4024be2c/.
### Baseline plus
baseline_evaluate.py evaluates the performance of a pretrained TSN using Baseline+'s 
fine-tuning methodology. The code for training a TSN can be found here:
https://github.com/liu-zhy/temporal-adaptive-module. 
It is necessary to clone this repository onto your machine, as Basline files require its 
functionality. Note that the actual temporal alignment 
module is not used here. For the pre-training of Baseline and Baseline Plus, most of the 
parameters are the same as the default parameters of TAM (wd=1e-4, batchsize 8x4, 
dropout 0.5). Set lr=1e-3 for all methods without using ImageNet pre-trained weights. 
When using ImageNet pre-trained weights, set lr=1e-4 for classifier-based methods and 
lr=1e-5 for meta-learning methods. The parameters can be specified in yaml files under 
baseline_config. The files in the baseline_plus directory implement 
some required functionality for baseline evaluation. Specifically, save_features is responsible for 
saving features generated by the model, while test.py takes care of adapting the trained model 
to the specific few shot task (see Baseline+ paper for more detail).
### Meta
meta.py either trains or evaluates meta-learning methods. This repository currently 
supports the meta-learning baseline and OTAM. Set parameters in meta/utils.py. meta.py 
contains a function for both training and testing, and it additionally instantiates the data loaders.
Finally, it runs the actual train or test. It has supporting files under the meta directory. Scores 
is generally responsible for computing the similarity between embedding sequences produced by the 
meta-learning methods. Utils is responsible for parsing arguments and loading the correct model. Lastly, 
meta_template.py defines a basic outline of the meta-learning methods used here.
### Dataset
dataset.py feeds data to the model in training. It includes a few classes, but all are 
geared towards loading frames from a video, providing labels, and feeding them to the model.
Unlike in Baseline+, I do not convert the SSv2 videos to frames, instead coding dataset.py 
to retrieve frames directly from videos. See the documentation for more detail on what the 
specific classes are actually for.